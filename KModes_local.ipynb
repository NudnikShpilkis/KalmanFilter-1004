{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def standardize(labels):\n",
    "#Standardize numerical feature\n",
    "    return (labels-labels.mean())/labels.std(ddof=0)\n",
    "\n",
    "def cat_encode(labels):\n",
    "    ids, new_labels = np.unique(labels, return_inverse=True)\n",
    "    return new_labels, ids\n",
    "    \n",
    "def mode2(a, axis=0, val=False):\n",
    "#Return mode or mode freq of an array\n",
    "#code based on scipy.stats.mode\n",
    "    scores = np.unique(np.ravel(a))      \n",
    "    testshape = list(a.shape)\n",
    "    testshape[axis] = 1\n",
    "    oldmostfreq = np.zeros(testshape, dtype=a.dtype)\n",
    "    oldcounts = np.zeros(testshape, dtype=int)\n",
    "\n",
    "    for score in scores:\n",
    "        template = (a == score)\n",
    "        counts = np.expand_dims(np.sum(template, axis),axis)\n",
    "        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n",
    "        oldcounts = np.maximum(counts, oldcounts)\n",
    "        oldmostfreq = mostfrequent\n",
    "    \n",
    "    return oldcounts[0] if val else mostfrequent[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('data/3nja-bsch.tsv', sep='\\t')\n",
    "a = a.round(1)\n",
    "#a2 = a.dropna().copy()\n",
    "a2 = a.copy()\n",
    "a2.reset_index(inplace=True, drop=True)\n",
    "for col in a2:\n",
    "    typ = str(a2[col].dtype)\n",
    "    if typ in ['int', 'int32', 'int64','float', 'float32', 'float64']:\n",
    "        a2[col] = standardize(a2[col])\n",
    "   \n",
    "\n",
    "a3 = a2.copy().dropna()\n",
    "a3.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('data/h3zm-ta5h.tsv', sep='\\t')\n",
    "b = b.round(1)\n",
    "b2 = b.dropna().copy()\n",
    "b2.reset_index(inplace=True, drop=True)\n",
    "bCodes = []\n",
    "for col in b2:\n",
    "    typ = str(b2[col].dtype)\n",
    "    if typ in ['int', 'int32', 'int64','float', 'float32', 'float64']:\n",
    "        b2[col] = standardize(b2[col])\n",
    "    elif typ in ['object', 'string']:\n",
    "        b2[col], fcodes = cat_encode(b2[col])\n",
    "        bCodes.append(fcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', 'My', 'name', 'stevie'], dtype='<U6')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cMsk = np.array([\"hello\", 'My', 'name', 'is', 'stevie', 'wonder'])\n",
    "missing = np.array([3,5])\n",
    "cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "#Feature subclass for better use with KModes\n",
    "    def __init__(self, featType, labels):\n",
    "        #values in array form and feature dtype\n",
    "        #self.vals = labels[~pd.isnull(labels)]\n",
    "        #self.missing = np.where(pd.isnull(labels))[0]\n",
    "        self.vals = labels\n",
    "        self.size = self.vals.shape[0]\n",
    "        self.type = featType\n",
    "        #unique values, their orig indices and counts\n",
    "        vals, inds, counts = np.unique(self.vals, return_inverse=True, return_counts=True)\n",
    "        #Feature dictionary for use in itialization\n",
    "        self.valDict = {v:c for v,c in  zip(vals, counts)}\n",
    "        #frequency of values in orig indices\n",
    "        self.valFreq = counts[inds]\n",
    "        #self.valFreqMiss = np.insert(counts[inds], self.missing, 0)\n",
    "        return\n",
    "    \n",
    "    def getCenter(self, cMsk):\n",
    "    #Get center of feature in given centroid\n",
    "        #Mean for num features\n",
    "        #msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "        msk = cMsk\n",
    "        if self.type==float:\n",
    "            return self.vals[msk].mean()\n",
    "        #Mode for cat features\n",
    "        else:\n",
    "            return mode2(self.vals[msk], val=False)        \n",
    "    \n",
    "    def getDissimilarity(self, cntr, cMsk=None):\n",
    "    #Given centroid center, get dissimilarity\n",
    "        #Numerical feature use euclidean distance\n",
    "        if self.type==float: \n",
    "            dissims = (self.vals - cntr)**2\n",
    "        else:\n",
    "            #Get Weight of points, from WHICH paper?\n",
    "            weights = (self.valFreq + self.valDict[cntr])/(self.valFreq * self.valDict[cntr])\n",
    "            #2007 dissimilarity\n",
    "            if cMsk is not None:\n",
    "                msk = cMsk\n",
    "                #msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "                dissims = weights * (1-(self.vals==cntr).astype(int) *((self.vals[msk] == cntr).sum()/msk.sum()))\n",
    "            #Classic dissimilarity\n",
    "            else:\n",
    "                dissims = weights * (self.vals != cntr).astype(int)\n",
    "        \n",
    "        return dissims\n",
    "        #dissims2 = np.insert(dissims, self.missing, 1000).astype(float)\n",
    "        #return dissims2\n",
    "        \n",
    "    def calcEntropy(self, cMsk):\n",
    "        msk = cMsk\n",
    "        #msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "        local_vals = self.vals[msk]\n",
    "        if local_vals.shape[0] == 1: return 0\n",
    "        \n",
    "        keys, counts_raw = np.unique(local_vals, return_counts=True)        \n",
    "        counts = counts_raw[counts_raw.nonzero()]\n",
    "        probs = counts/counts.sum() \n",
    "\n",
    "        label_entropy = -np.sum(probs * np.log(probs)) \n",
    "        entropy_weight = 2*(1 - 1/(1 + np.exp(-label_entropy)))\n",
    "        return entropy_weight * label_entropy\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "def initIndivCentroid(features, info, cntrd_id):\n",
    "    cntrd_tmp = [0 for i in range(info['n_ftrs'])]\n",
    "\n",
    "    for f, feat in enumerate(features):\n",
    "        f_id = cntrd_id - np.sum(feat.missing < cntrd_id)\n",
    "        if f_id in feat.missing:\n",
    "            cntrd_tmp[f] = feat.getCenter(np.ones(feat.size, dtype=bool))\n",
    "        else:\n",
    "            cntrd_tmp[f] = feat.vals[f]\n",
    "\n",
    "    return cntrd_tmp\n",
    "\n",
    "def initCentroids(features, info):\n",
    "#Init centroids based on Cao Paper\n",
    "    #Empty list of centroids\n",
    "    cntrds = [0 for i in range(info['n_clstrs'])]\n",
    "    #Density of each point\n",
    "    dens = np.zeros(info['n_rows'])\n",
    "    #Iterate through features\n",
    "    for feat in features:\n",
    "        #Update point density\n",
    "        dens += (feat.valFreq/(info['n_rows']*info['n_clstrs']))\n",
    "    cntrds[0] = [feat.vals[dens.argmax()] for feat in features]\n",
    "    #Iterate though remaining centroids, adapted from github implementation \n",
    "    for ki in range(1, info['n_clstrs']):\n",
    "        adj_dens = np.zeros((ki, info['n_rows']))\n",
    "        #Iterate through already created centroids\n",
    "        for kii in range(0,ki):\n",
    "            #Similarity btwn cntrds and pts\n",
    "            dissims = np.zeros((info['n_rows'], info['n_ftrs']))\n",
    "            #dissims = np.zeros(info['n_rows'])\n",
    "            #Iterate through features\n",
    "            for f, feat in enumerate(features):\n",
    "                dissims[:,f] += feat.getDissimilarity(cntrds[kii][f]).astype(float)\n",
    "            #Densities * similarites\n",
    "            adj_dens[kii] = dens * dissims.sum(axis=1)\n",
    "        #New centroid has max dens*sims from previous centroids\n",
    "        print(np.argmax(np.min(adj_dens, axis=0)))\n",
    "        cntrds[ki] = [feat.vals[np.argmax(np.min(adj_dens, axis=0))] for feat in features]\n",
    "    #Assign intial membship\n",
    "    cntrds, membship, cost = assignMembship(cntrds, features, info)\n",
    "    return cntrds, membship, cost\n",
    "\n",
    "def assignMembship(cntrds, features, info, membship=None, pred=True):\n",
    "#Assign membship to centroids, membship=True if dissim is 2007, pred=True for KModes predict\n",
    "    dissims = np.zeros((info['n_rows'], info['n_clstrs']))\n",
    "    #Iterate through centroids\n",
    "    for i, cntrd in enumerate(cntrds):\n",
    "        #Get num and cat feat sims  \n",
    "        num_feats = np.zeros((info['n_rows'], info['num']))\n",
    "        cat_feats = np.zeros((info['n_rows'], info['cat']))\n",
    "        n, c = 0, 0\n",
    "        #Iterate thorugh features\n",
    "        for f, feat in enumerate(features):\n",
    "            #Num features\n",
    "            if isinstance(feat.type, (float, int)):\n",
    "                num_feats[:,n] += feat.getDissimilarity(cntrd[f]).astype(float)\n",
    "                n += 1\n",
    "            #Cat geatures\n",
    "            else:\n",
    "                #Choose which dissim to use\n",
    "                if membship is not None:\n",
    "                    #2007 dissim\n",
    "                    cntrdMask = (membship == i)\n",
    "                    cat_feats[:,c] += feat.getDissimilarity(cntrd[f], cntrdMask).astype(float)\n",
    "                else:\n",
    "                    #Orig disim\n",
    "                    cat_feats[:,c] += feat.getDissimilarity(cntrd[f]).astype(float)\n",
    "                c += 1\n",
    "        #Sims of given centroid\n",
    "        dissims[:,i] = num_feats.sum(axis=1) + info['gamma'] * cat_feats.sum(axis=1)\n",
    "    #Choose smallest dissim for each pt\n",
    "    membship = dissims.argmin(axis=1)\n",
    "    #Cost is sum of smallest dissim\n",
    "    cost = dissims.min(axis=1).sum()\n",
    "    #If centroid has no assigned pts\n",
    "    if (np.unique(membship).shape[0] != info['n_clstrs'])&(not pred):\n",
    "        print('meep meep')\n",
    "        cntrds = newCentroids(cntrds, features, membship, info)\n",
    "        #Recursively assign membship\n",
    "        cntrds, membship, cost = assignMemship(cntrds, features, info, membship)\n",
    "    return cntrds, membship, cost\n",
    "\n",
    "def newCentroids(cntrds, features, membship, info):\n",
    "#Create new centroids if centroid is not present in membship\n",
    "    #Missing centroids\n",
    "    missing = np.setdiff1d(range(0,info['n_clstrs']), membship)\n",
    "    #Largest centroid\n",
    "    max_cntrd = mode2(membship, val=False)\n",
    "    #Choose pts from largest centroid\n",
    "    new_cntrds = np.random.choice(np.where(membship == max_cntrd)[0], missing.shape[0])\n",
    "    #Replace old centroids with new points\n",
    "    for m, nc in zip(missing, new_cntrds):\n",
    "        cntrds[m] = [feat.vals[new_cntrds[nc]] for feat in features]\n",
    "    return cntrds\n",
    "\n",
    "def setCentroids(features, membship, info, dissim):\n",
    "#Set centroids from center of each feature\n",
    "    cntrds = [0 for c in range(info['n_clstrs'])]\n",
    "    #Iterate through centroids, get approp center for each feature\n",
    "    for i in range(info['n_clstrs']):\n",
    "        cntrds[i] = [feat.getCenter(membship==i) for feat in features]\n",
    "    #2007 dissim, pass membship\n",
    "    if dissim:\n",
    "        cntrds, membship, cost = assignMembship(cntrds, features, info, membship)\n",
    "    #Classsic dissim\n",
    "    else:\n",
    "        cntrds, membship, cost = assignMembship(cntrds, features, info)\n",
    "    return cntrds, membship, cost\n",
    " \n",
    "def setMembshipToMatrix(membship, info):\n",
    "#Convert membship to 2D array\n",
    "    labels = np.zeros((info['n_rows'], info['n_clstrs']))\n",
    "    for r, cls, in enumerate(membship):\n",
    "        labels[r, cls] += 1\n",
    "    return labels\n",
    "\n",
    "def doGWE(features, msk):\n",
    "    assigned = np.where(msk)[0]\n",
    "    tot_e = np.sum([feat.calcEntropy(msk) for feat in features]) #calc initial weighted-entropy\n",
    "    o_factors = np.array([tot_e for x in range(assigned.shape[0])])\n",
    "    for o, i in enumerate(assigned):\n",
    "        msk[i] = False\n",
    "        wo = np.sum([feat.calcEntropy(msk) for feat in features])\n",
    "        o_factors[o]  -= wo if wo != 0 else (tot_e + 1)\n",
    "        msk[i] = True\n",
    "    msk_msk = (o_factors > 0) \n",
    "    pts = np.zeros(msk.shape[0], dtype=bool)\n",
    "    pts[assigned] = msk_msk\n",
    "    return np.arange(msk.shape[0])[pts]\n",
    "\n",
    "def getOutliers(features, membship, info):\n",
    "    outliersList =  [0 for i in range(info['n_clstrs'])]\n",
    "    \n",
    "    for i in range(info['n_clstrs']):\n",
    "        outliersList[i] = doGWE(features, membship == i)\n",
    "        \n",
    "    return np.array(list(itertools.chain.from_iterable(outliersList)))   \n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "class KModes(BaseEstimator, ClusterMixin):\n",
    "#Implementation of KModes, with help from github implementation\n",
    "    def __init__(self, n_clusters=5, max_iter=100, gamma=1, dissim=False):\n",
    "        #Keep all size information in dictionary\n",
    "        self.info = {}\n",
    "        self.info['n_clstrs'], self.info['max_itr'] = n_clusters, max_iter\n",
    "        self.info['gamma'] = 1\n",
    "        #2007 dissim or classic dissim\n",
    "        self.dissim = dissim\n",
    "        return     \n",
    "    \n",
    "    def fit(self, X, types):\n",
    "    #Fit KModes to data\n",
    "        #Convert Pandas to np array\n",
    "        if 'pandas' in str(X.__class__):\n",
    "            X = X.values\n",
    "        \n",
    "        self.info['n_rows'], self.info['n_ftrs'] = X.shape\n",
    "        #Too few clusters\n",
    "        assert self.info['n_clstrs'] <= self.info['n_rows']\n",
    "        #Features to better form\n",
    "        self.features = np.array([Feature(types[i], X[:,i]) for i in range(X.shape[1])])\n",
    "        #Save n_ftrs and c_ftrs\n",
    "        self.info['num'] = int(np.sum([1 for feat in self.features if isinstance(feat.type, (float, int))]))\n",
    "        self.info['cat'] = int(self.info['n_ftrs'] - self.info['num'])\n",
    "        \n",
    "        #Init centroids, membship and cost\n",
    "        cntrds, membship, cost = initCentroids(self.features, self.info)\n",
    "    \n",
    "        #Iterate until convergence or max iteration reached\n",
    "        itr, converged = 0, False\n",
    "        while (itr <= self.info['max_itr'])&(not converged):\n",
    "            itr += 1\n",
    "            #Set centroids, membship\n",
    "            cntrds, new_membship, new_cost = setCentroids(self.features, membship, self.info, self.dissim)\n",
    "            #Check if any pts changed membship\n",
    "            chngs = np.sum(new_membship != membship)\n",
    "            #Convergence conditions\n",
    "            converged = (chngs == 0)|(new_cost >= cost)\n",
    "            membship, cost = new_membship, new_cost\n",
    "        \n",
    "        #Save fit info\n",
    "        self.outliers_ = getOutliers(self.features, membship, self.info)\n",
    "        self.membship_ = membship\n",
    "        self.matrix_ = setMembshipToMatrix(membship, self.info)\n",
    "        self.centroids_, self.cost_, self.itr_ = cntrds, cost, itr\n",
    "        return self   \n",
    "    \n",
    "    def predict(self, X):\n",
    "    #Given new data, assignMembship to centroids\n",
    "        assert hasattr(self, 'centroids_'), \"Model not yet fit\"\n",
    "        if 'pandas' in str(X.__class__):\n",
    "            X = X.values\n",
    "        new_features = np.array([Feature(col.dtype, col) for col in X.T])\n",
    "        new_info = self.info.copy()\n",
    "        new_info[pts] = X.shape[0]\n",
    "        cntrds, membship, cost = assignMembship(self.centroids_, new_features, new_info, pred=True)\n",
    "        new_labels = setMembshipToLabels(membship, info)\n",
    "        return new_labels, cost\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        return self.fit(X).predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "8\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "a_types = [str, float, float, float, float, float, float, str, str, float, float, float, float, float, float, float, float, float, float, float, float, float]\n",
    "ninjaClusters = KModes(n_clusters=int(np.sqrt(a3.shape[0]/2))).fit(a3, a_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b_types = [float, str, float, float, str, float, float, float, float, float, float, float, float, str, float, float]\n",
    "h3zmClusters = KModes(n_clusters=int(np.sqrt(b2.shape[0]/2))).fit(b2, b_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 40])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.outliers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([152, 166, 248, 262, 264, 346, 444, 458, 183, 185, 191, 485, 187,\n",
       "        72,  74, 154, 464, 466, 104, 202, 300, 398])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3zmClusters.outliers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "#Feature subclass for better use with KModes\n",
    "    def __init__(self, featType, labels):\n",
    "        #values in array form and feature dtype\n",
    "        self.vals = labels[~pd.isnull(labels)]\n",
    "        self.missing = np.where(pd.isnull(labels))[0]\n",
    "        self.size = self.vals.shape[0]\n",
    "        self.type = featType\n",
    "        #unique values, their orig indices and counts\n",
    "        vals, inds, counts = np.unique(self.vals, return_inverse=True, return_counts=True)\n",
    "        #Feature dictionary for use in itialization\n",
    "        self.valDict = {v:c for v,c in  zip(vals, counts)}\n",
    "        #frequency of values in orig indices\n",
    "        self.valFreq = counts[inds]\n",
    "        self.valFreqMiss = self.fillMissing(self.valFreq)\n",
    "        return\n",
    "    \n",
    "    def fillMissing(self, arr):\n",
    "        for miss in self.missing:\n",
    "            arr = np.concatenate((arr[:miss], [np.nan], arr[miss:]))\n",
    "        return arr\n",
    "    \n",
    "    def getCenter(self, cMsk):\n",
    "    #Get center of feature in given centroid\n",
    "        #Mean for num features\n",
    "        msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "        if self.type==float:\n",
    "            return self.vals[msk].mean()\n",
    "        #Mode for cat features\n",
    "        else:\n",
    "            return mode2(self.vals[msk], val=False)        \n",
    "    \n",
    "    def getDissimilarity(self, cntr, cMsk=None):\n",
    "    #Given centroid center, get dissimilarity\n",
    "        #Numerical feature use euclidean distance\n",
    "        if self.type==float: \n",
    "            dissims = (self.vals - cntr)**2\n",
    "            dissims = self.fillMissing(dissims)\n",
    "            return dissims.astype(np.float64)\n",
    "        else:\n",
    "            #Get Weight of points, from WHICH paper?\n",
    "            weights = (self.valFreq + self.valDict[cntr])/(self.valFreq * self.valDict[cntr])\n",
    "            #2007 dissimilarity\n",
    "            if cMsk is not None:\n",
    "                msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "                dissims = weights * (1-(self.vals==cntr).astype(int) *((self.vals[msk] == cntr).sum()/msk.sum()))\n",
    "                dissims = self.fillMissing(dissims)\n",
    "                return dissims.astype(np.float64)\n",
    "            #Classic dissimilarity\n",
    "            else:\n",
    "                dissims = weights * (self.vals != cntr).astype(int)\n",
    "                dissims = self.fillMissing(dissims)\n",
    "                return dissims.astype(np.float64)\n",
    "                \n",
    "    def calcEntropy(self, cMsk):\n",
    "        msk = cMsk[np.setdiff1d(np.arange(cMsk.shape[0]), self.missing)]\n",
    "        local_vals = self.vals[msk]\n",
    "        if local_vals.shape[0] == 1: return 0\n",
    "        \n",
    "        keys, counts_raw = np.unique(local_vals, return_counts=True)        \n",
    "        counts = counts_raw[counts_raw.nonzero()]\n",
    "        probs = counts/counts.sum() \n",
    "\n",
    "        label_entropy = -np.sum(probs * np.log(probs)) \n",
    "        entropy_weight = 2*(1 - 1/(1 + np.exp(-label_entropy)))\n",
    "        return entropy_weight * label_entropy\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "def initIndivCentroid(features, info, cntrd_id):\n",
    "    cntrd_tmp = [0 for i in range(info['n_ftrs'])]\n",
    "\n",
    "    for f, feat in enumerate(features):\n",
    "        f_id = cntrd_id - np.sum(feat.missing < cntrd_id)\n",
    "        if f_id in feat.missing:\n",
    "            cntrd_tmp[f] = feat.getCenter(np.ones(feat.size, dtype=bool))\n",
    "        else:\n",
    "            cntrd_tmp[f] = feat.vals[f_id]\n",
    "    \n",
    "    return cntrd_tmp\n",
    "\n",
    "def initCentroids(features, info):\n",
    "#Init centroids based on Cao Paper\n",
    "    #Empty list of centroids\n",
    "    cntrds = [0 for i in range(info['n_clstrs'])]\n",
    "    #Density of each point\n",
    "    dens = np.zeros(info['n_rows'])\n",
    "    #Iterate through features\n",
    "    for feat in features:\n",
    "        #Update point density\n",
    "        dens += (feat.valFreqMiss/(info['n_rows']*info['n_clstrs']))\n",
    "    #cntrds[0] = [feat.vals[dens.argmax()] for feat in features]\n",
    "    cntrds[0] = initIndivCentroid(features, info, np.nanargmax(dens))\n",
    "    #Iterate though remaining centroids, adapted from github implementation \n",
    "    for ki in range(1, info['n_clstrs']):\n",
    "        adj_dens = np.zeros((ki, info['n_rows']))\n",
    "        #Iterate through already created centroids\n",
    "        for kii in range(0,ki):\n",
    "            #Similarity btwn cntrds and pts\n",
    "            dissims = np.zeros((info['n_rows'], info['n_ftrs']))\n",
    "            #dissims = np.zeros(info['n_rows'])\n",
    "            #Iterate through features\n",
    "            for f, feat in enumerate(features):\n",
    "                dissims[:,f] += feat.getDissimilarity(cntrds[kii][f])\n",
    "            #Densities * similarites\n",
    "            adj_dens[kii] = (dens * dissims.sum(axis=1)).astype(np.float64)\n",
    "        #New centroid has max dens*sims from previous centroids\n",
    "        print(np.nanargmax(np.nanmin(adj_dens.astype(np.float64), axis=0)))\n",
    "        #cntrds[ki] = initIndivCentroid(features, info, np.nanargmax(np.nanmin(adj_dens, axis=0)))\n",
    "        cntrds[ki] = [feat.vals[np.nanargmax(np.nanmin(adj_dens.astype(np.float64), axis=0))] for feat in features]\n",
    "    #Assign intial membship\n",
    "    cntrds, membship, cost = assignMembship(cntrds, features, info)\n",
    "    return cntrds, membship, cost\n",
    "\n",
    "def assignMembship(cntrds, features, info, membship=None, pred=True):\n",
    "#Assign membship to centroids, membship=True if dissim is 2007, pred=True for KModes predict\n",
    "    dissims = np.zeros((info['n_rows'], info['n_clstrs']))\n",
    "    #Iterate through centroids\n",
    "    for i, cntrd in enumerate(cntrds):\n",
    "        #Get num and cat feat sims  \n",
    "        num_feats = np.zeros((info['n_rows'], info['num']))\n",
    "        cat_feats = np.zeros((info['n_rows'], info['cat']))\n",
    "        n, c = 0, 0\n",
    "        #Iterate thorugh features\n",
    "        for f, feat in enumerate(features):\n",
    "            #Num features\n",
    "            if isinstance(feat.type, (float, int)):\n",
    "                num_feats[:,n] += feat.getDissimilarity(cntrd[f])\n",
    "                n += 1\n",
    "            #Cat geatures\n",
    "            else:\n",
    "                #Choose which dissim to use\n",
    "                if membship is not None:\n",
    "                    #2007 dissim\n",
    "                    cntrdMask = (membship == i)\n",
    "                    cat_feats[:,c] += feat.getDissimilarity(cntrd[f], cntrdMask)\n",
    "                else:\n",
    "                    #Orig disim\n",
    "                    cat_feats[:,c] += feat.getDissimilarity(cntrd[f])\n",
    "                c += 1\n",
    "        #Sims of given centroid\n",
    "        dissims[:,i] = np.nansum(num_feats, axis=1) + info['gamma'] * np.nansum(cat_feats, axis=1)\n",
    "    #Choose smallest dissim for each pt\n",
    "    membship = np.nanargmin(dissims, axis=1)\n",
    "    #Cost is sum of smallest dissim\n",
    "    cost = np.nanmin(dissims, axis=1).sum()\n",
    "    #If centroid has no assigned pts\n",
    "    if (np.unique(membship).shape[0] != info['n_clstrs'])&(not pred):\n",
    "        print('meep meep')\n",
    "        cntrds = newCentroids(cntrds, features, membship, info)\n",
    "        #Recursively assign membship\n",
    "        cntrds, membship, cost = assignMemship(cntrds, features, info, membship)\n",
    "    return cntrds, membship, cost\n",
    "\n",
    "def newCentroids(cntrds, features, membship, info):\n",
    "#Create new centroids if centroid is not present in membship\n",
    "    #Missing centroids\n",
    "    missing = np.setdiff1d(range(0,info['n_clstrs']), membship)\n",
    "    #Largest centroid\n",
    "    max_cntrd = mode2(membship, val=False)\n",
    "    #Choose pts from largest centroid\n",
    "    new_cntrds = np.random.choice(np.where(membship == max_cntrd)[0], missing.shape[0])\n",
    "    #Replace old centroids with new points\n",
    "    for m, nc in zip(missing, new_cntrds):\n",
    "        cntrds[m] = [feat.vals[new_cntrds[nc]] for feat in features]\n",
    "    return cntrds\n",
    "\n",
    "def setCentroids(features, membship, info, dissim):\n",
    "#Set centroids from center of each feature\n",
    "    cntrds = [0 for c in range(info['n_clstrs'])]\n",
    "    #Iterate through centroids, get approp center for each feature\n",
    "    for i in range(info['n_clstrs']):\n",
    "        cntrds[i] = [feat.getCenter(membship==i) for feat in features]\n",
    "    #2007 dissim, pass membship\n",
    "    if dissim:\n",
    "        cntrds, membship, cost = assignMembship(cntrds, features, info, membship)\n",
    "    #Classsic dissim\n",
    "    else:\n",
    "        cntrds, membship, cost = assignMembship(cntrds, features, info)\n",
    "    return cntrds, membship, cost\n",
    " \n",
    "def setMembshipToMatrix(membship, info):\n",
    "#Convert membship to 2D array\n",
    "    labels = np.zeros((info['n_rows'], info['n_clstrs']))\n",
    "    for r, cls, in enumerate(membship):\n",
    "        labels[r, cls] += 1\n",
    "    return labels\n",
    "\n",
    "def doGWE(features, msk):\n",
    "    assigned = np.where(msk)[0]\n",
    "    tot_e = np.sum([feat.calcEntropy(msk) for feat in features]) #calc initial weighted-entropy\n",
    "    o_factors = np.array([tot_e for x in range(assigned.shape[0])])\n",
    "    for o, i in enumerate(assigned):\n",
    "        msk[i] = False\n",
    "        wo = np.sum([feat.calcEntropy(msk) for feat in features])\n",
    "        o_factors[o]  -= wo if wo != 0 else (tot_e + 1)\n",
    "        msk[i] = True\n",
    "    msk_msk = (o_factors > 0) \n",
    "    pts = np.zeros(msk.shape[0], dtype=bool)\n",
    "    pts[assigned] = msk_msk\n",
    "    return np.arange(msk.shape[0])[pts]\n",
    "\n",
    "def getOutliers(features, membship, info):\n",
    "    outliersList =  [0 for i in range(info['n_clstrs'])]\n",
    "    \n",
    "    for i in range(info['n_clstrs']):\n",
    "        outliersList[i] = doGWE(features, membship == i)\n",
    "        \n",
    "    return np.array(list(itertools.chain.from_iterable(outliersList)))   \n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "class KModes(BaseEstimator, ClusterMixin):\n",
    "#Implementation of KModes, with help from github implementation\n",
    "    def __init__(self, n_clusters=5, max_iter=100, gamma=1, dissim=False):\n",
    "        #Keep all size information in dictionary\n",
    "        self.info = {}\n",
    "        self.info['n_clstrs'], self.info['max_itr'] = n_clusters, max_iter\n",
    "        self.info['gamma'] = 1\n",
    "        #2007 dissim or classic dissim\n",
    "        self.dissim = dissim\n",
    "        return     \n",
    "    \n",
    "    def fit(self, X, types):\n",
    "    #Fit KModes to data\n",
    "        #Convert Pandas to np array\n",
    "        if 'pandas' in str(X.__class__):\n",
    "            X = X.values\n",
    "        \n",
    "        self.info['n_rows'], self.info['n_ftrs'] = X.shape\n",
    "        #Too few clusters\n",
    "        assert self.info['n_clstrs'] <= self.info['n_rows']\n",
    "        #Features to better form\n",
    "        self.features = np.array([Feature(types[i], X[:,i]) for i in range(X.shape[1])])\n",
    "        #Save n_ftrs and c_ftrs\n",
    "        self.info['num'] = int(np.sum([1 for feat in self.features if isinstance(feat.type, (float, int))]))\n",
    "        self.info['cat'] = int(self.info['n_ftrs'] - self.info['num'])\n",
    "        \n",
    "        #Init centroids, membship and cost\n",
    "        cntrds, membship, cost = initCentroids(self.features, self.info)\n",
    "    \n",
    "        #Iterate until convergence or max iteration reached\n",
    "        itr, converged = 0, False\n",
    "        while (itr <= self.info['max_itr'])&(not converged):\n",
    "            itr += 1\n",
    "            #Set centroids, membship\n",
    "            cntrds, new_membship, new_cost = setCentroids(self.features, membship, self.info, self.dissim)\n",
    "            #Check if any pts changed membship\n",
    "            chngs = np.sum(new_membship != membship)\n",
    "            #Convergence conditions\n",
    "            converged = (chngs == 0)|(new_cost >= cost)\n",
    "            membship, cost = new_membship, new_cost\n",
    "        \n",
    "        #Save fit info\n",
    "        self.outliers_ = getOutliers(self.features, membship, self.info)\n",
    "        self.membship_ = membship\n",
    "        self.matrix_ = setMembshipToMatrix(membship, self.info)\n",
    "        self.centroids_, self.cost_, self.itr_ = cntrds, cost, itr\n",
    "        return self   \n",
    "    \n",
    "    def predict(self, X):\n",
    "    #Given new data, assignMembship to centroids\n",
    "        assert hasattr(self, 'centroids_'), \"Model not yet fit\"\n",
    "        if 'pandas' in str(X.__class__):\n",
    "            X = X.values\n",
    "        new_features = np.array([Feature(col.dtype, col) for col in X.T])\n",
    "        new_info = self.info.copy()\n",
    "        new_info[pts] = X.shape[0]\n",
    "        cntrds, membship, cost = assignMembship(self.centroids_, new_features, new_info, pred=True)\n",
    "        new_labels = setMembshipToLabels(membship, info)\n",
    "        return new_labels, cost\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        return self.fit(X).predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "8\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "a_types = [str, float, float, float, float, float, float, str, str, float, float, float, float, float, float, float, float, float, float, float, float, float]\n",
    "ninjaClusters = KModes(n_clusters=int(np.sqrt(a2.shape[0]/2))).fit(a2, a_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 0, 3, 3, 3, 3, 2, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 1, 1, 3,\n",
       "       3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.membship_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.8571407399789"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.cost_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12, 25, 40, 41, 19, 20, 39, 42])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.outliers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.73558140744552"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.cost_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 40])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninjaClusters.outliers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
